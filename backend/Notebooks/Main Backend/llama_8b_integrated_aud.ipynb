{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 152905,
     "status": "ok",
     "timestamp": 1739855580458,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "EJfEAVGOWq9k"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install -U accelerate\n",
    "!pip install -U peft\n",
    "!pip install -U trl\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 40485,
     "status": "ok",
     "timestamp": 1739855620939,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "VUkM_nGjZnlL"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "executionInfo": {
     "elapsed": 18742,
     "status": "ok",
     "timestamp": 1739855743802,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "_hhZYONTaS1j",
    "outputId": "47c1a7d6-dc3b-487e-eccb-c32d7c140638"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlucyfursinahag\u001b[0m (\u001b[33mlucyfursinahag-vit-bhopal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250218_051535-5v3gyg8i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5v3gyg8i?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">ruby-frost-19</a></strong> to <a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5v3gyg8i?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5v3gyg8i?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "hf_token=userdata.get('hft')\n",
    "\n",
    "\n",
    "\n",
    "login(token = hf_token)\n",
    "\n",
    "wb_token = userdata.get('wandb')\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 8B on Medical Dataset',\n",
    "    job_type=\"training\",\n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739855750092,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "raaIPIjYcTxs"
   },
   "outputs": [],
   "source": [
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Update with your local path in Colab\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "new_model = \"llama-3-8b-med-tuner\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739855751185,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "0caSd5j6e-WN"
   },
   "outputs": [],
   "source": [
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aaf7b238a8ff4904a0b3f22ccfe25654",
      "4d30bf05336145df854695c3d928947f",
      "093a5026099c4375b868c8ffaea59ae5",
      "4dead8388d6f40c1b1631eafdd76ba57",
      "8382b2f038a44b9c82cc67845c8fdb29",
      "ad50acf4e20045d0ba4843372eb05d61",
      "8636c3c5e0114628b059053e9850641e",
      "cfe08fa67e3645feb3f7b2b7b8ca7817",
      "799fdcb069f24c78a2d852ec4cae412f",
      "b45b9dfbc5a84e68a070e0f4826baf4d",
      "3db288baa3054352a6cfc171f72824c0",
      "380a7c2143f14b3faed9f17730a2f1f7",
      "1895be1c0f28451ab3753b98aeb19d3f",
      "b3be6223b5444aaa905795946c7c40dc",
      "f903435e7c9545749ce0c1b21bc66e96",
      "afc1b18fd63247a7b9e061dda16be972",
      "98cbdb4bf36c4c499dece7a4fa545f63",
      "5e5784360a0c43a7909504152c2a41fc",
      "197a5aa29c2641d19bf42e6a60d32689",
      "e6a5de3988164c4cb7515e2fd7ad37af",
      "a7437fa0b27d403baaff465c1a9276a1",
      "b4731856721d4a7995e957b74d9ed1b3",
      "edcb2767dcfe4cccad1486d41b0ea571",
      "ec0fd4fe32f441f7a22fc034ceab1070",
      "dbc53fc92b0841b98f12a0c39958aa35",
      "8b40c04065294e5c89d643e934f30f08",
      "27e6bf8d48b5460a945eb99e11a280c5",
      "fcf9d427c2bc4d65a950ba330466b71e",
      "9a968286d8e44b7a88ba5937d504b05d",
      "f169ef819660453588f935a0d59b631c",
      "3f364ad6bb9447159d6c5a63c69af2a7",
      "66314c39017f46d99db0e02cc423a43c",
      "06c785f00df34cc68be5a7c77d292264",
      "d3582d90723c4da88140e085ae037ceb",
      "121a61d4f1ce4cd5a2c29d13cb9ba25f",
      "4cd4922677f845b1b051b210f2c65f9f",
      "7195100b527e4c648ccbdf7a7e484925",
      "009edf3a2f2a4bfd861109c8e354d1fa",
      "2f273a21908945ddaab3ccd8354e0b6c",
      "fed6e985c1ec4c29aa40b9bd90a7e16a",
      "84144a9c5eed41858ea4c1ad4850bce2",
      "aa8980137f1b4b25ba5c46180664b63a",
      "cb54bcbf8e174ffda089aa66f5cc9d27",
      "fc75c7a409b74584b62af51e376c4684",
      "cf95843effae491dae89327e191f246e",
      "916a4f90c6f04915a7a0b68ab9007e3b",
      "eceb748d8c6346cfb78913dced5ebfad",
      "fab87d0904aa478fa511a3bc13e7b8b9",
      "8b36192ad7d34e6793ba006de0b80b5c",
      "010bf3d6634943a1a9b4d23720c00a5f",
      "60c53cc2acf54dbda41ce0d12208d510",
      "f90db817e55146c397c1ff373bc6e02c",
      "412a53202e1a42d8af1c0692e881f1e3",
      "45ff8a1055604aeabe07d3e71c67100d",
      "da157757f7e142dca1a5d24a97d4535c",
      "cc24ad9b97cb45cf922fcd21c6cf253c",
      "f353fb67c432439a967f95ea6e980dbd",
      "a46800a85b4b46cc9c14b574a7641769",
      "2f406fcb11cb4c2db3445f1124943e6e",
      "1fab241866ef49cb8aa6a93f92d2a954",
      "dc408e74ff1047f182e090cdbbb224c0",
      "c2a2774fd6d246cf98909e372871a90f",
      "f4266ba0af0748f6a6d7c1df9c084555",
      "c741edadb905429ca024d172ed77221e",
      "6b49f7be19734f2095078f112b8ef3df",
      "614abb277b0e417c82d09728fdbdaea2",
      "03a6739b8ff54c5eb840bfa9290e1afb",
      "cc9dd61151644a1d804c9318f5cbaffd",
      "2301ad2a28894e9ea07676166b352d8c",
      "647b7c6a0ffb4d4b85b7f186d28ab252",
      "aedbe3d617e0475c8ec04fe502358204",
      "9248f403b4304c499e9ea5624e275142",
      "776f52c40ec849b6911d2469d75dad61",
      "eea785cb4419489895a6952d1a35a249",
      "ceeb53404d024db99f69843b62b8c78f",
      "0ee96accd5444eafabcec99dd32e4607",
      "950a8dbc8c854a50b2f13da362ebf1f5",
      "671afa65e7c341adb864157fb520edc6",
      "a6e9376020414d12ba6eb9f107f8905b",
      "96c07e0242c443b79f7b028d6bbcac24",
      "2d36029019644cbb90fb6cc7b41a4ab2",
      "e57f21407c3a443f8d99168672ddb99f",
      "58f9e08e7cf2469c858deb0a07fdac66",
      "e4d2a5d60ef441f1aebde61a5c3f5b9b",
      "5779a3b82cae4cfdbce79a9531f9607d",
      "27c4ec8124624b1c9afdc21cdc28b8cc",
      "3fbafad0e11547c9be9f456ad2686865",
      "d9477a533371487a8c80c54dc050168a",
      "3354552df6cd4443aa147053d0d29554",
      "6d23437c424c439595fddc9665622947",
      "cf7d4628fe5c4044b448c4da90b1df7c",
      "cccafa25e82144da84698090a57bc7cb",
      "6e7fafb761b543c88062d8e2868922af",
      "81a84f2b74284cf4acf39102731f4a2b",
      "db22b9d5c78340e48912cc1162429a68",
      "1ce70bf90cd0416aac171d7fa2b7cd85",
      "ae8dac5115734763bcd422fcacaea0cc",
      "84422888316940cd8b2ea0b6234066d7",
      "910bdbb0c63446ecbb4f8c7f71a079d2"
     ]
    },
    "executionInfo": {
     "elapsed": 476981,
     "status": "ok",
     "timestamp": 1739856229799,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "2rCHh9yPfOSw",
    "outputId": "424f42b7-9aa0-46f7-e089-040940a74d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf7b238a8ff4904a0b3f22ccfe25654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380a7c2143f14b3faed9f17730a2f1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcb2767dcfe4cccad1486d41b0ea571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3582d90723c4da88140e085ae037ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf95843effae491dae89327e191f246e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc24ad9b97cb45cf922fcd21c6cf253c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a6739b8ff54c5eb840bfa9290e1afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671afa65e7c341adb864157fb520edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3354552df6cd4443aa147053d0d29554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n",
    "!pip install --upgrade transformers timm\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    "\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "813251869d2947e6a192aee6b8900823",
      "e64e272265034008b4d2ec08d413d9c7",
      "9b8bb986ede34c779c1c47831b0996d0",
      "283fa469ced9456990626e97b6b7215e",
      "88b8e59d675049bdbb117fc520fab05e",
      "b822d5c0a7a74404bed90e6ab57096c1",
      "ea200f4364ee4a6eaf251865635f72c7",
      "4ca28059d1614f04bf2dce32aa427b4d",
      "b19cf35ca0404692a7b5a7e31870fabd",
      "4596a8130c374023bf280d8798616c85",
      "c2e212d4fa024ea1984032948121b032",
      "586316a5e0e54fa18113cb067b38b65e",
      "2f1c66fe9ce64317a8df70d7fe9ab39b",
      "0baa053c77c34beca96035973b15bdb0",
      "c6a7bab1753146a08af135843592472d",
      "d3080ed5cc7c48a4beed327fe6d0b4a9",
      "ed24e727af234e2f85572c9a052c3364",
      "0e204ca9b36d4406b21fdda380de6553",
      "cbfd490d515b4fe38e0590c84a66149d",
      "5b89a567ecfc477ca434f45899ddad3c",
      "d04cb9c837134dddb27c494472200bd8",
      "86b9ab338aeb42b8aff7fc680df64c05",
      "8c44daad1eb9473cb734362acf8cb265",
      "ad51cf6216374713b328045017b531ab",
      "a10c88ef0bfa4292a2cd38c405279429",
      "8b6e99ccc5fe407cb09f96f81cec7c17",
      "8ceb1181cc254aac954158a3bf08b89a",
      "4f42d7e1d15b431faba8e4a67663a857",
      "9d0122fdcc40417d8b391f6db8d86ce8",
      "f1a32a54f35e4ca7b417607fff603ebb",
      "fff3b54929484fd7b57ffeaf9b9d4440",
      "280a0a2c853842519bf0a3dfa7abcd62",
      "65aca243716b4f08b002f46ac2f9f593"
     ]
    },
    "executionInfo": {
     "elapsed": 23620,
     "status": "ok",
     "timestamp": 1739856253411,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "11qd3oZffRiI",
    "outputId": "6427ebab-d79d-4b45-abe8-f5b22e951f20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813251869d2947e6a192aee6b8900823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586316a5e0e54fa18113cb067b38b65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c44daad1eb9473cb734362acf8cb265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1739856253411,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "kVd9NozdfVRJ"
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285,
     "referenced_widgets": [
      "289dbddfdb0641ef8d1456d3f6ef6806",
      "7493c469387945c5b34903c917965638",
      "815a62fb0e2a49058db396995c868d58",
      "72551da11e6c4ae6be58747de4284c65",
      "cf0e3a496ba245dda151cbbc318d5233",
      "c1695f5994d54a4aa5ae500455789f52",
      "c5b7e4c899cc4ce0bd6380a7a618d8ab",
      "8c247257f8ea47418cfeb881e9cb7e81",
      "4cdd363e92f6474c9241ce6d0c2e874f",
      "5f98d1e8f4a444febc81389abc405960",
      "cec908e1d319488cbadb5a67c8b2bdba",
      "f392fb073db04e1995ead0f20b2306c9",
      "e95cc521df6f453d91f5e8870c97b5c3",
      "6f059858d45f4bf6aa6d4cf4c2f60912",
      "a2aab79012ff4f34aba92750ccead924",
      "082508aae4fe4e11b0dd2ba49fb8751f",
      "192256f9fc6d42c8bef76d4ad5947a72",
      "ecd52a6142944f4c82093b6c9f710e5c",
      "d821287208584d5d9002928c17adc8db",
      "e8e42ac7754547158bb7a7d3de8b14b7",
      "44e9100e49c841229c3ddeba8a75fc60",
      "54103f55cd564d2fb2a3e17df91b499e",
      "9537745b8db54c6f9ec3d64238096cb2",
      "82f0b9ecb2a8473f80502dbc65200916",
      "16f78c98b503443b8014c58b1082c235",
      "bea8186f02d14a57ba2a2bd09218b6de",
      "b02e8efcfa784bbf990c2d0da41eea0b",
      "7e96680509d34cce98448dd3f9732f4a",
      "f01a29770838463f9d95a351ab87ab73",
      "3b496990e6784155bfc5d301b99e0698",
      "048c5eabff294fc88e46c13b8d1e9630",
      "17fc9d863b684f75b0522e973be739d9",
      "74e356c879d9444199b2d2b12e7bd136",
      "038e9cfc797f4629b2f9e2d4ced078b0",
      "280e01d3938c48498fbd1d6a2b37a708",
      "cf722ce48cc5432db60ab745cf47d10e",
      "1e209c23d89b4adc8c0031bd1ed63ebb",
      "433d4e90b01c447ea775b1feb138bffc",
      "d77ee488bdfd4b769b7f6cd4fb9487f8",
      "8a303200c4b143ce9648bb07f38b5097",
      "3c6b4b7ad294424eb61693ad23aae526",
      "35f7f9dde5b34ab1a237c98f32d604f0",
      "6793e7d70c404c9d9dbe62e6eac83c20",
      "8ee2a39a17fd4e7796ea57068c3cb39b"
     ]
    },
    "executionInfo": {
     "elapsed": 2279,
     "status": "ok",
     "timestamp": 1739856255680,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "ZBI8rF0XnowY",
    "outputId": "85b65eb6-df90-4b88-e670-d22d97aa23a7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289dbddfdb0641ef8d1456d3f6ef6806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/863 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f392fb073db04e1995ead0f20b2306c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dialogues.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9537745b8db54c6f9ec3d64238096cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038e9cfc797f4629b2f9e2d4ced078b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1739856255681,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "44GAn2p8oSXF"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1739856255681,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "KJ8vtC-corlb"
   },
   "outputs": [],
   "source": [
    "new_model=\"llama-8b-med-tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1739856255681,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "XdrpLj4NoU5C",
    "outputId": "3abecb2b-e327-416c-f29e-ede8be721099"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    gradient_checkpointing=True\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "c9a54c051960418c944ad29891610ad4",
      "9af392a1e3994d26a0dedc9f7d9c4b3e",
      "deaf7543909241c8a26bc3914fbe8585",
      "a86cd6cbd66948319344603d7f0f21c8",
      "c65464abf05d4b9d855a0639f089ccf3",
      "d5a5f9bfd6de4b5f8e24638fbab65ef2",
      "0ce4e4cf029743599785e353b7d4c01f",
      "b18e20f325274cd1af97ee083d824ad6",
      "4e1e15903aae4ab2ad8d4734cca1aec4",
      "6fe7beaf8d2646beab742829646067b1",
      "e936cadf3ff04727935db82384c55e7a",
      "0dfff3958f2145248e4c03666406e584",
      "d4d6d54d623a4571bd2e889447b62156",
      "cab92e5b3ed347f6a19f46a12989eaf5",
      "b1ae419abb7143a8878fbe16cdac4d7d",
      "d4c692574001471f94090382d6899540",
      "26a501c0594e4c7f9fe2888ba4383063",
      "b1903abddfd64251b9d943c957f21251",
      "c166a27a9e184f3da56c33b7370048d6",
      "6c26f43cd4504e3384e26137dfa98bd4",
      "fd600b1b17a74500bab97064aebfa61f",
      "7e94cbbadf754a5cbb097173980070d3",
      "47f80af5062542339bdf43d633f2e7d7",
      "f1edcbc5e8bf42258911975f729d0235",
      "0357bf3141dc453dbc07a9ac046f5b50",
      "d0036a610feb42ba991a80f12bfe33b8",
      "d76c582631384435ac5518ccd912b23e",
      "e1e7d91840b6409ca3ef02f0216aec0a",
      "b00b171b65fc41109df9d0eb84f2444f",
      "5501769b353046a6bae1ee58aed83c11",
      "9df81382ca2c4029b4f498cc232fee21",
      "69cd6baef6e643d5993a4a448b73dc06",
      "1ceaacb468674143b4141c6ca38ac6b0",
      "7b76efe3d053472f92aac5d35b7128ae",
      "6ed3dd06d8084659822c061832fb0ed5",
      "32235504c224488a866a37935503ad45",
      "233cfea3d8594b4ab02fb36e3f12ff84",
      "55cb13dea3b548d192f1c2ce31aa4071",
      "cf06d3f0c7b84783a671e09db82b5660",
      "8d5b48fa5bcb4711ba18f1e7f2af1751",
      "850c9a311a5d453b8fd97209cd489db6",
      "76bed6b51af94ad38b6266ddb8db551f",
      "b6f97acb4b4044eca09c7c10031b8c87",
      "ca2bea1578d948458e06b92750033367",
      "b4943c9fd289443289f49ad026a869c0",
      "4ae938c136cf43d68349af177c928c6f",
      "19485431cdbe4ec5842480c11f1b2601",
      "223dc39406ae450bbbfea9de37520d3e",
      "0a9d25f78dfa4ab1b84546a88c7c0cee",
      "1279a48220a6498b811dc259851e2737",
      "646957702246463baf224aee6c47313a",
      "e94e044f23094788b0fa5e44a04bf8e5",
      "ea19b12d92bf4edaaea4a2c3b2690d00",
      "3f41335fdff7441f8ddbfc45594db694",
      "c718b31d7bf1420797fd6c9c9e1eea8f",
      "c35c6d278df04f95b7192ea8ce9aab18",
      "0f7a4a1242da43c4ad46917097bf52bf",
      "871a502e9e234daab669a97aae58620c",
      "e3123704f73447d498b19bcf308f9c29",
      "9c8dfa9a87684241ba5e24418f6dc254",
      "51b772d7fe2947e0a43b799720344c9a",
      "90828a8c682e46f7823dad2d4cf99dba",
      "5715a40a1c0244d588c152c855f0eff6",
      "57929891f2fe44a7b2cbdbe1d1c828f7",
      "3772748116134102955a770780fb74a3",
      "729ddce712d24d89bf8d67ca798798a7"
     ]
    },
    "executionInfo": {
     "elapsed": 4140,
     "status": "ok",
     "timestamp": 1739856405765,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "pkDcNITro3pG",
    "outputId": "7bf7ba2d-dab7-4b13-d040-bdd025a1ae1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-44bfd1628395>:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a54c051960418c944ad29891610ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfff3958f2145248e4c03666406e584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f80af5062542339bdf43d633f2e7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b76efe3d053472f92aac5d35b7128ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4943c9fd289443289f49ad026a869c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35c6d278df04f95b7192ea8ce9aab18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    #max_seq_length=512,\n",
    "    #dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    #packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739856407806,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "PHvezGE2uz1j",
    "outputId": "e4dbc818-5183-46a3-ef68-98f091fe9bde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 4096)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding_side=\"right\") # padding side set to right to be consistent with chat template\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Ensure the model is in training mode before starting training\n",
    "#model.train() # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739856410996,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "51ppvrmo1EPO",
    "outputId": "3d06a689-3f11-4dcf-cc7e-98363d41f6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n",
      "Parameter does not require gradients\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(\"Parameter does not require gradients\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739856416361,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "aKLTTU8L2dYS"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and not torch.is_floating_point(param.data):\n",
    "        print(f\"Parameter {name} has invalid dtype: {param.data.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1739856418637,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "PvInNWAV1RFY",
    "outputId": "45a93974-49e1-4154-f49e-4cf3b54cabf2"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-df3504271e60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1739856423445,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "8dHfrP172ugz",
    "outputId": "d9631a59-8448-49fd-cd6a-5c1c39db2eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128257, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1852512,
     "status": "ok",
     "timestamp": 1739858280337,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "Egc8K_vNuRz8",
    "outputId": "b26204ae-41f2-42a1-aba9-2ecf78e54551"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 30:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.911200</td>\n",
       "      <td>2.675745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.420600</td>\n",
       "      <td>2.665204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.359000</td>\n",
       "      <td>2.609890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.555500</td>\n",
       "      <td>2.567071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.314800</td>\n",
       "      <td>2.542778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=2.521121243370904, metrics={'train_runtime': 1849.8417, 'train_samples_per_second': 0.487, 'train_steps_per_second': 0.487, 'total_flos': 9345230984871936.0, 'train_loss': 2.521121243370904})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "executionInfo": {
     "elapsed": 2299,
     "status": "ok",
     "timestamp": 1739858282607,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "g3di6ini3B_7",
    "outputId": "9cdee8d3-3629-4c8b-bcbd-53aff1bdc71b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▅▂▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▁▄▆█</td></tr><tr><td>eval/runtime</td><td>█▆▁▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▄█▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▄█▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▂▁▁▁▅▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▇████▇▇▇▇▆▆▆▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▇▆▆▄▃▇▁█▅▆▅▆▄▇▃▆▄▆▇▆▆▅▃▇█▅█▆▇▂▇▆▇▅▆▆▄▇▄▅</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▄▃▂▃▂▃▁▃▂▄▃▁▃▂▄▁▂▂▅█▅▂▃▁▂▂▄▃▄▃▄▃▃▂▂▄▃█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.54278</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.45691</td></tr><tr><td>eval/runtime</td><td>45.9363</td></tr><tr><td>eval/samples_per_second</td><td>2.177</td></tr><tr><td>eval/steps_per_second</td><td>2.177</td></tr><tr><td>total_flos</td><td>9345230984871936.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>900</td></tr><tr><td>train/grad_norm</td><td>5.40691</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.3148</td></tr><tr><td>train/mean_token_accuracy</td><td>0.45872</td></tr><tr><td>train_loss</td><td>2.52112</td></tr><tr><td>train_runtime</td><td>1849.8417</td></tr><tr><td>train_samples_per_second</td><td>0.487</td></tr><tr><td>train_steps_per_second</td><td>0.487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-frost-19</strong> at: <a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5v3gyg8i?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/5v3gyg8i?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95</a><br> View project at: <a href='https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95' target=\"_blank\">https://wandb.ai/lucyfursinahag-vit-bhopal/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset?apiKey=f9c0b0dee535d7659607eb9de0238fe95f570f95</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250218_051535-5v3gyg8i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z19DV2bfgUs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 42203,
     "status": "ok",
     "timestamp": 1739858713992,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "blwwPrFbf15M",
    "outputId": "ce07eee2-7926-4ee3-ad6f-8da60c35b5f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-378f9024-2742-4539-85b3-06ae15f14c41\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-378f9024-2742-4539-85b3-06ae15f14c41\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving faster_whisper_module.py to faster_whisper_module.py\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28251,
     "status": "ok",
     "timestamp": 1739858742239,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "p3KN-tYhgJTu",
    "outputId": "4866deac-a8f9-49d4-b59e-c456d7db28ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.5/39.5 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "import faster_whisper_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "referenced_widgets": [
      "e90da6b0a00e4c728093db175fea8850",
      "7c550ce0465b4565b35d38495a672f22",
      "fd1531bc3aef42219a0c3b5ac3dbaf5a",
      "ab49597cab6543b784f613ac5dcfca49",
      "8bd5ad48fdcb49d4a268a3a63660b503",
      "d69e365eeab04c15a7e390436aaeeab4",
      "fe24e7766b2b46a39db0a7edd5dbca37",
      "428b9df694754251aa9d37ba95698912",
      "095fd0d6b3a043368faae4ee6e5070e1",
      "59077d321990496f99916ff552ac68d8",
      "7997cf9653344e4d807f9cd20c8b2b60",
      "297027f31226405b9da95cc54087e7a3",
      "d9d47ab0ae504d6cbb084eaadb4697f9",
      "d82eda56f1234e51af09dcacef2fe940",
      "54b0c34a0fde4be0be73cf66babd19eb",
      "2d1b222d893b413f86a1436237301ab8",
      "5e494943b4c24fa1b19f5df2253ccc94",
      "df53fa74564e489581528c1ceb21dd4d",
      "bdbb288f41a64f7aa880493d15f15a46",
      "47f4aa74aa3a460184bf635b3cf38184",
      "4a81d36c0423468389e302f3cc432d55",
      "ebbb723c3a8a4c8f85e2ee1c79bb3030",
      "5eecb01e0c76447e94f61a4d34e446a9",
      "14f2f2ff2682484e836c17c5017ae199",
      "58bed05a53414d7d9c1dfe910f6e6eb7",
      "a4ca0d2f6b6347dbb190dcf118c39bb8",
      "577a50fcbabe4fedb37fa0bf15d983a4",
      "7ce9a60ef48347ea80c6c614476b5b42",
      "a636497b770344ba98ba7181945f0959",
      "1ee96f549eeb4f088f0fef6a1c2373a0",
      "20c84fdc5da54dc5932db82b0aedc278",
      "51ac06368c5b49e6a602efca1daeb994",
      "60d33745fb4e4f7d9db1d7f1a89fe878",
      "24e690769f664d43a42208aefe8700df",
      "6ea1a09905fb4855ae2a039f408df752",
      "649447407f094bd1942834eb54fa551d",
      "53bde3bcc10044e080b20f027584064e",
      "e9f28d27338e454a95e2124cebab5a50",
      "2baf71aed946411ebac44d196f36a7fc",
      "d8e88aa55ee34207985df949e79ab8c9",
      "18c5024917264617a4cf24d9381322a6",
      "aa8e008475274161a49c65117fe1b886",
      "507fdccb50e54c3f96a37d259d867a44",
      "9f4facbe0162484db41c0dd65f5659aa"
     ]
    },
    "executionInfo": {
     "elapsed": 67598,
     "status": "ok",
     "timestamp": 1739858941925,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "oR19xNwdgM8m",
    "outputId": "48e86a77-662a-4b21-c9b5-9b99a2b60cb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-ffcbd260-8f3a-449a-aed3-49fbefa8b36d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-ffcbd260-8f3a-449a-aed3-49fbefa8b36d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving test_audio001.wav to test_audio001.wav\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90da6b0a00e4c728093db175fea8850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297027f31226405b9da95cc54087e7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eecb01e0c76447e94f61a4d34e446a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/145M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e690769f664d43a42208aefe8700df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocabulary.txt:   0%|          | 0.00/460k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Transcribing...\n",
      " Maintain proper hydration and rest for recovery.\n"
     ]
    }
   ],
   "source": [
    "trans=faster_whisper_module.process_audio()\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23125,
     "status": "ok",
     "timestamp": 1739858975214,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "dvVMysg6qXjv",
    "outputId": "6b43b97f-f6b2-43d5-f8e3-17d8383e2dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maintain proper hydration and rest for recovery.\n",
      "enter the query: i have frequent fever and pain in chest\n",
      " Maintain proper hydration and rest for recovery.i have frequent fever and pain in chest\n"
     ]
    }
   ],
   "source": [
    "print(trans)\n",
    "query=input(\"enter the query: \")\n",
    "t=trans+query\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20074,
     "status": "ok",
     "timestamp": 1739859000848,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "igf6stcb3I0I",
    "outputId": "c8d54e14-6542-4e92-9191-da0e8f836725"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hello, I have gone through your query. From what you have mentioned, it seems that you are having a dry cough. I would suggest you to use some cough syrups like Tab Robitussin or Tab Mucodyne for 3-5 days. Also, you can use some expectorant like Tab Mucopain for 3-5 days. If you have a fever, then you can use some antipyretic like Tab Paracetamol for 3-5 days. Hope I have answered your query. Let me know if I can assist you further. Regards, Dr. Shinas Hussain, General & Family Physician\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"t\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True,\n",
    "                   truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150,\n",
    "                         num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123864,
     "status": "ok",
     "timestamp": 1739859733513,
     "user": {
      "displayName": "Rana Talukdar",
      "userId": "04433901076618612995"
     },
     "user_tz": -330
    },
    "id": "0cyMSy1s3Krn",
    "outputId": "c44bf2c4-726b-4916-831c-2bfe3709df5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "#trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFXNbhdYTAc6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
